{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FineTune_Marian_From_English2Romanian_to_Macedonian2Romanian.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers sentencepiece sacrebleu==1.5.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8Qe5Cd4D7_0",
        "outputId": "0a486217-9731-4191-b7d0-63f09efa93f7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.18.1-py3-none-any.whl (311 kB)\n",
            "\u001b[K     |████████████████████████████████| 311 kB 8.5 MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 66.1 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 58.1 MB/s \n",
            "\u001b[?25hCollecting sacrebleu==1.5.1\n",
            "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting portalocker==2.0.0\n",
            "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 58.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.10.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 67.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 40.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 53.0 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 64.8 MB/s \n",
            "\u001b[?25hCollecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 73.8 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 74.5 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 56.2 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pyyaml, fsspec, aiohttp, xxhash, tokenizers, sacremoses, portalocker, huggingface-hub, transformers, sentencepiece, sacrebleu, datasets\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-1.18.1 frozenlist-1.3.0 fsspec-2022.1.0 huggingface-hub-0.4.0 multidict-6.0.2 portalocker-2.0.0 pyyaml-6.0 sacrebleu-1.5.1 sacremoses-0.0.47 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.15.0 xxhash-2.0.2 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4qUIBNCFDTJL"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mk_ro_train = load_dataset(\"setimes\", \"mk-ro\", split='train[0:10000]')\n",
        "mk_ro_val = load_dataset(\"setimes\", \"mk-ro\", split='train[10000:12500]')\n",
        "mk_ro_test = load_dataset(\"setimes\", \"mk-ro\", split='train[12500:15000]')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxBgaocPDifN",
        "outputId": "e388380a-6b1a-496f-9e97-fc9894dd3a01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset setimes (/root/.cache/huggingface/datasets/setimes/mk-ro/1.0.0/5b0222bb707caa9d423c61813ef94861e1ccdf82fa4b0bdf4a98de3c9fd33d0d)\n",
            "Reusing dataset setimes (/root/.cache/huggingface/datasets/setimes/mk-ro/1.0.0/5b0222bb707caa9d423c61813ef94861e1ccdf82fa4b0bdf4a98de3c9fd33d0d)\n",
            "Reusing dataset setimes (/root/.cache/huggingface/datasets/setimes/mk-ro/1.0.0/5b0222bb707caa9d423c61813ef94861e1ccdf82fa4b0bdf4a98de3c9fd33d0d)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mk_ro_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM4MS9PUJIh3",
        "outputId": "6d929898-11d3-468e-e74c-bcc1378c0495"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'translation'],\n",
              "    num_rows: 10000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mk_ro_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxeOYXyBKaEc",
        "outputId": "7345f5e1-061e-4eba-8497-596176c7b107"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'translation'],\n",
              "    num_rows: 2500\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mk_ro_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xo94TlwHKaB5",
        "outputId": "abf03cc2-1a20-4c33-a282-a00ee89ceb31"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'translation'],\n",
              "    num_rows: 2500\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mk_ro_train[\"translation\"][:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9yhs6ppGmpp",
        "outputId": "1d50e7cd-9efa-4d41-e561-daac644443bd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'mk': 'Продолжува кипарскиот застој, додека претседавањето со ЕУ се наближува',\n",
              "  'ro': 'Impasul din Cipru continuă în timp ce preşedinţia UE se apropie'},\n",
              " {'mk': 'Додека ОН ги притиска кипарските Грци и Турци да најдат решение за поделениот остров, малкумина очекуваат чудо пред кипарските Грци да го преземат претседавањето со ЕУ во јули.',\n",
              "  'ro': 'În timp ce ONU îi presează pe ciprioţii greci şi turci să găsească o soluţie pentru insula divizată, puţini se aşteaptă la un miracol înainte ca ciprioţii greci să preia preşedinţia UE în iulie.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ro\")"
      ],
      "metadata": {
        "id": "hhW8c0l-DiV4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"\"\n",
        "max_input_length = 128\n",
        "max_target_length = 128\n",
        "source_lang = \"mk\"\n",
        "target_lang = \"ro\"\n",
        "\n",
        "def preprocess_function(sentences):\n",
        "    inputs = [prefix + sentence[source_lang] for sentence in sentences[\"translation\"]]\n",
        "    targets = [sentence[target_lang] for sentence in sentences[\"translation\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "rzb5JuZKGCah"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_function(mk_ro_train[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOmc9XnfDiTw",
        "outputId": "5a9eaf64-49a4-415d-804a-c1f85b2cbec0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[51409, 54126, 16974, 14207, 13303, 45931, 18255, 15963, 15138, 15, 14440, 11272, 29426, 37639, 8379, 14440, 11272, 14207, 21208, 55271, 32023, 14207, 55664, 3, 15, 16974, 14207, 16974, 11829, 49625, 53927, 54352, 8379, 11829, 16974, 15138, 15963, 15138, 55729, 11829, 53372, 15, 8379, 14207, 15, 41726, 52502, 15, 8379, 11829, 53815, 32648, 51107, 45931, 18255, 15963, 15138, 0], [15, 39202, 14207, 16974, 11829, 49625, 15, 31595, 39759, 15, 29371, 11272, 53927, 11272, 21208, 11272, 8379, 49625, 15, 14440, 11272, 29426, 37639, 8379, 14440, 11272, 49466, 15, 45930, 18554, 38274, 11272, 15, 11272, 15, 39721, 18255, 18554, 38274, 11272, 15, 16974, 15138, 53815, 55664, 16974, 15138, 21208, 15, 18554, 11829, 49350, 33480, 11272, 11829, 55271, 15, 29426, 14207, 16974, 11829, 13303, 33480, 11272, 14207, 21208, 15, 14207, 32023, 18554, 33725, 3, 15, 54631, 13303, 14440, 18255, 18004, 11272, 29546, 15, 14207, 37205, 11829, 14440, 18255, 15963, 15138, 15138, 21208, 15, 37205, 18255, 16974, 14207, 53927, 11829, 16974, 15, 14440, 11272, 29426, 37639, 8379, 14440, 11272, 49466, 15, 45930, 18554, 38274, 11272, 15, 16974, 15138, 15, 29371, 14207, 53927, 11829, 26784, 11829, 54631, 21208, 53927, 54352, 8379, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[9855, 841, 62, 44, 16558, 2967, 17, 300, 52, 166, 1588, 8206, 266, 631, 59, 5973, 0], [127, 300, 52, 8076, 583, 10102, 527, 38, 30637, 22, 69, 221, 25857, 31, 12316, 22, 19, 7463, 27, 2200, 210, 39, 11835, 27184, 71, 3, 622, 22, 59, 3233, 24, 42, 267, 13558, 563, 70, 30637, 22, 69, 221, 25857, 19, 14326, 166, 1588, 8206, 266, 631, 17, 2558, 2, 0]]}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train = mk_ro_train.map(preprocess_function, batched=True)\n",
        "tokenized_val = mk_ro_val.map(preprocess_function, batched=True)\n",
        "tokenized_test = mk_ro_test.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsdWNYIpDiRd",
        "outputId": "cd43b4b8-9e2a-42e2-cbee-029d987ae500"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/setimes/mk-ro/1.0.0/5b0222bb707caa9d423c61813ef94861e1ccdf82fa4b0bdf4a98de3c9fd33d0d/cache-ecaf7c0454fd5a59.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/setimes/mk-ro/1.0.0/5b0222bb707caa9d423c61813ef94861e1ccdf82fa4b0bdf4a98de3c9fd33d0d/cache-9d391781f9698952.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/setimes/mk-ro/1.0.0/5b0222bb707caa9d423c61813ef94861e1ccdf82fa4b0bdf4a98de3c9fd33d0d/cache-842a5a03915c644a.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-ro\")"
      ],
      "metadata": {
        "id": "2V2odAUxDiPG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "model_name = \"opus-mt-en-ro\"\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=10,\n",
        "    predict_with_generate=True    \n",
        ")\n",
        "\n",
        "metric = load_metric(\"sacrebleu\")\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "W9LSyUYjDiMs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\n",
        "        \"bleu\": result[\"score\"] \n",
        "    }\n",
        "\n",
        "    prediction_lens = [\n",
        "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
        "    ]\n",
        "\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {\n",
        "        k: round(v, 4) for k, v in result.items()\n",
        "    }\n",
        "    return result"
      ],
      "metadata": {
        "id": "rkYyXoDFDiKo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "_QmBgEOMDiIA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W-yq-YCoDiFq",
        "outputId": "92e15b48-ebfe-4699-ec7d-b7e865cce6c2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: id, translation.\n",
            "***** Running training *****\n",
            "  Num examples = 10000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 6250\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6250/6250 1:04:02, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.624200</td>\n",
              "      <td>2.813786</td>\n",
              "      <td>9.383200</td>\n",
              "      <td>36.864800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.840200</td>\n",
              "      <td>2.462099</td>\n",
              "      <td>14.281800</td>\n",
              "      <td>34.660000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.475600</td>\n",
              "      <td>2.291636</td>\n",
              "      <td>17.470200</td>\n",
              "      <td>34.253200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.101200</td>\n",
              "      <td>2.195141</td>\n",
              "      <td>19.441300</td>\n",
              "      <td>33.962400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.954600</td>\n",
              "      <td>2.130142</td>\n",
              "      <td>21.052600</td>\n",
              "      <td>33.516400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.857500</td>\n",
              "      <td>2.084888</td>\n",
              "      <td>22.085000</td>\n",
              "      <td>33.176800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.787900</td>\n",
              "      <td>2.065709</td>\n",
              "      <td>22.751500</td>\n",
              "      <td>32.690800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.677700</td>\n",
              "      <td>2.048159</td>\n",
              "      <td>23.498100</td>\n",
              "      <td>32.991600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.636800</td>\n",
              "      <td>2.038754</td>\n",
              "      <td>23.708700</td>\n",
              "      <td>32.846400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.622300</td>\n",
              "      <td>2.036164</td>\n",
              "      <td>23.662100</td>\n",
              "      <td>32.860000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-500\n",
            "Configuration saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-500/config.json\n",
            "Model weights saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-500/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: id, translation.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2500\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-1000\n",
            "Configuration saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-1000/config.json\n",
            "Model weights saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-1000/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: id, translation.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2500\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-1500\n",
            "Configuration saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-1500/config.json\n",
            "Model weights saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-1500/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: id, translation.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2500\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-2000\n",
            "Configuration saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-2000/config.json\n",
            "Model weights saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-2000/special_tokens_map.json\n",
            "Deleting older checkpoint [opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-500] due to args.save_total_limit\n",
            "Saving model checkpoint to opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-2500\n",
            "Configuration saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-2500/config.json\n",
            "Model weights saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-2500/pytorch_model.bin\n",
            "tokenizer config file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-2500/special_tokens_map.json\n",
            "Deleting older checkpoint [opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-1000] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: id, translation.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2500\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-3000\n",
            "Configuration saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-3000/config.json\n",
            "Model weights saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-3000/special_tokens_map.json\n",
            "Deleting older checkpoint [opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-1500] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: id, translation.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2500\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-3500\n",
            "Configuration saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-3500/config.json\n",
            "Model weights saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-3500/pytorch_model.bin\n",
            "tokenizer config file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-3500/tokenizer_config.json\n",
            "Special tokens file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-3500/special_tokens_map.json\n",
            "Deleting older checkpoint [opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-2000] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: id, translation.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2500\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-4000\n",
            "Configuration saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-4000/config.json\n",
            "Model weights saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-4000/special_tokens_map.json\n",
            "Deleting older checkpoint [opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-2500] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: id, translation.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2500\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-4500\n",
            "Configuration saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-4500/config.json\n",
            "Model weights saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-4500/pytorch_model.bin\n",
            "tokenizer config file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-4500/tokenizer_config.json\n",
            "Special tokens file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-4500/special_tokens_map.json\n",
            "Deleting older checkpoint [opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-3000] due to args.save_total_limit\n",
            "Saving model checkpoint to opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5000\n",
            "Configuration saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5000/config.json\n",
            "Model weights saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5000/pytorch_model.bin\n",
            "tokenizer config file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5000/tokenizer_config.json\n",
            "Special tokens file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5000/special_tokens_map.json\n",
            "Deleting older checkpoint [opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-3500] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: id, translation.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2500\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5500\n",
            "Configuration saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5500/config.json\n",
            "Model weights saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5500/pytorch_model.bin\n",
            "tokenizer config file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5500/tokenizer_config.json\n",
            "Special tokens file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5500/special_tokens_map.json\n",
            "Deleting older checkpoint [opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-4000] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: id, translation.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2500\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000\n",
            "Configuration saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/config.json\n",
            "Model weights saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/pytorch_model.bin\n",
            "tokenizer config file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/tokenizer_config.json\n",
            "Special tokens file saved in opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/special_tokens_map.json\n",
            "Deleting older checkpoint [opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-4500] due to args.save_total_limit\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: id, translation.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2500\n",
            "  Batch size = 16\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=6250, training_loss=2.1104263427734375, metrics={'train_runtime': 3843.2517, 'train_samples_per_second': 26.02, 'train_steps_per_second': 1.626, 'total_flos': 3389645814497280.0, 'train_loss': 2.1104263427734375, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "# src_text = ['Во таа група се наоfаат пеесет и пет земји, вклучувајќи ги и сите земји-членки на ЕУ.']\n",
        "src_text = ['Хрватска е рангирана на 48-мото место со HDI од 0,830, и е единствена земја од Југоисточна Европа која се наоfа во групата земји со висок човеков развој.']\n",
        "model_name = 'opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000'\n",
        "\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n",
        "[tokenizer.decode(t, skip_special_tokens=True) for t in translated]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNNuejYIDiDS",
        "outputId": "5527fdae-620c-428c-f8fe-cd2f6b15ce7c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Didn't find file opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/added_tokens.json. We won't load it.\n",
            "Didn't find file opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/tokenizer.json. We won't load it.\n",
            "loading file opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/source.spm\n",
            "loading file opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/target.spm\n",
            "loading file opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/vocab.json\n",
            "loading file opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/tokenizer_config.json\n",
            "loading file None\n",
            "loading file opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/special_tokens_map.json\n",
            "loading file None\n",
            "loading configuration file opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/config.json\n",
            "Model config MarianConfig {\n",
            "  \"_name_or_path\": \"Helsinki-NLP/opus-mt-en-ro\",\n",
            "  \"_num_labels\": 3,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"swish\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"MarianMTModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bad_words_ids\": [\n",
            "    [\n",
            "      59542\n",
            "    ]\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_attention_heads\": 8,\n",
            "  \"decoder_ffn_dim\": 2048,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 59542,\n",
            "  \"dropout\": 0.1,\n",
            "  \"encoder_attention_heads\": 8,\n",
            "  \"encoder_ffn_dim\": 2048,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 0,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_length\": 512,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"marian\",\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 59542,\n",
            "  \"scale_embedding\": true,\n",
            "  \"static_position_embeddings\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 59543\n",
            "}\n",
            "\n",
            "loading weights file opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing MarianMTModel.\n",
            "\n",
            "All the weights of MarianMTModel were initialized from the model checkpoint at opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Croaţia s-a situat pe locul 48 cu HID de 0,30 şi este o ţară vecină din Europa de Sud-est, care se află în ţările din grup cu un nivel înalt de dezvoltare.']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mk_ro_test[\"translation\"][:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcR-a0mTDiA6",
        "outputId": "5910b113-bdb4-4387-fd9a-26a7af2cbb98"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'mk': 'Хрватска е рангирана на 48-мото место со HDI од 0,830, и е единствена земја од Југоисточна Европа која се наоfа во групата земји со висок човеков развој.',\n",
              "  'ro': 'Croaţia, plasată pe locul al 48-lea cu un HDI de 0,830, este singura ţară sud-est europeană din grupul dezvoltării umane înalte.'},\n",
              " {'mk': 'Во таа група се наоfаат пеесет и пет земји, вклучувајќи ги и сите земји-членки на ЕУ.',\n",
              "  'ro': 'Cincizeci şi cinci de ţări se află în acest grup, inclusiv toate statele membre UE.'},\n",
              " {'mk': 'Другите земји од Југоисточна Европа се рангирани во групата средно развиени земји (HDI од 0,500 до 0,799).',\n",
              "  'ro': 'Celelalte ţări sud-est europene sunt plasate în grupul de dezvoltare umană medie-înaltă (HDI de la 0,500 la 0,799).'},\n",
              " {'mk': 'Бугарија се наоfа на 56-тото место со HDI од 0,796, по што следи Македонија, која е рангирана на 60-тото место со HDI од 0,793.',\n",
              "  'ro': 'Bulgaria se plasează pe locul al 56-lea cu un HDI de 0,796, fiind urmată de Macedonia, clasată a 60-a cu un HDI de 0,793.'},\n",
              " {'mk': 'Со еднакви вредности на HDI од 0,781, Албанија и Босна и Херцеговина се најдоа на 65-тото, односно 66-тото место.',\n",
              "  'ro': 'Având valori egale ale HDI de 0,781, Albania şi Bosnia şi Herţegovina sunt plasate pe locul al 65-lea şi respectiv al 66-lea.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r opus-mt-en-ro-finetuned-mk-to-ro.zip opus-mt-en-ro-finetuned-mk-to-ro/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKAZWTmsk-oF",
        "outputId": "914562e1-9883-4605-d0d9-bab343fe7966"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/ (stored 0%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/ (stored 0%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/tokenizer_config.json (deflated 40%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/vocab.json (deflated 70%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/pytorch_model.bin (deflated 7%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/training_args.bin (deflated 49%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/optimizer.pt (deflated 8%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/config.json (deflated 60%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/trainer_state.json (deflated 78%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/special_tokens_map.json (deflated 34%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/target.spm (deflated 49%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/source.spm (deflated 48%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/rng_state.pth (deflated 27%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/scheduler.pt (deflated 49%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5500/ (stored 0%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5500/tokenizer_config.json (deflated 40%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5500/vocab.json (deflated 70%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5500/pytorch_model.bin (deflated 7%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5500/training_args.bin (deflated 49%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5500/optimizer.pt (deflated 8%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5500/config.json (deflated 60%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5500/trainer_state.json (deflated 77%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5500/special_tokens_map.json (deflated 34%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5500/target.spm (deflated 49%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5500/source.spm (deflated 48%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5500/rng_state.pth (deflated 27%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5500/scheduler.pt (deflated 49%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/runs/ (stored 0%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/runs/Jan26_22-28-23_8b4d942eaaea/ (stored 0%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/runs/Jan26_22-28-23_8b4d942eaaea/1643236109.447152/ (stored 0%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/runs/Jan26_22-28-23_8b4d942eaaea/1643236109.447152/events.out.tfevents.1643236109.8b4d942eaaea.456.1 (deflated 62%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/runs/Jan26_22-28-23_8b4d942eaaea/events.out.tfevents.1643236109.8b4d942eaaea.456.0 (deflated 63%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/runs/Jan26_21-56-38_8b4d942eaaea/ (stored 0%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/runs/Jan26_21-56-38_8b4d942eaaea/1643234218.7148445/ (stored 0%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/runs/Jan26_21-56-38_8b4d942eaaea/1643234218.7148445/events.out.tfevents.1643234218.8b4d942eaaea.72.1 (deflated 62%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/runs/Jan26_21-56-38_8b4d942eaaea/events.out.tfevents.1643234218.8b4d942eaaea.72.0 (deflated 58%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/runs/Jan26_22-14-19_8b4d942eaaea/ (stored 0%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/runs/Jan26_22-14-19_8b4d942eaaea/1643235277.9386232/ (stored 0%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/runs/Jan26_22-14-19_8b4d942eaaea/1643235277.9386232/events.out.tfevents.1643235277.8b4d942eaaea.308.1 (deflated 62%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/runs/Jan26_22-14-19_8b4d942eaaea/events.out.tfevents.1643235277.8b4d942eaaea.308.0 (deflated 58%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5000/ (stored 0%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5000/tokenizer_config.json (deflated 40%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5000/vocab.json (deflated 70%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5000/pytorch_model.bin (deflated 7%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5000/training_args.bin (deflated 49%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5000/optimizer.pt (deflated 8%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5000/config.json (deflated 60%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5000/trainer_state.json (deflated 77%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5000/special_tokens_map.json (deflated 34%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5000/target.spm (deflated 49%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5000/source.spm (deflated 48%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5000/rng_state.pth (deflated 27%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-5000/scheduler.pt (deflated 49%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r opus-mt-en-ro-finetuned-mk-to-ro_checkpoint-6000.zip opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpBW17Of4j4Z",
        "outputId": "e14e32a5-c577-4ce2-cc24-21d7fad70efe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/ (stored 0%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/tokenizer_config.json (deflated 40%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/vocab.json (deflated 70%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/pytorch_model.bin (deflated 7%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/training_args.bin (deflated 49%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/optimizer.pt (deflated 8%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/config.json (deflated 60%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/trainer_state.json (deflated 78%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/special_tokens_map.json (deflated 34%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/target.spm (deflated 49%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/source.spm (deflated 48%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/rng_state.pth (deflated 27%)\n",
            "  adding: opus-mt-en-ro-finetuned-mk-to-ro/checkpoint-6000/scheduler.pt (deflated 49%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bg -> En -> Mk -> Ro\n",
        "# Grk -> En -> Mk -> Ro\n",
        "# Sr -> En -> Mk -> Ro"
      ],
      "metadata": {
        "id": "c5Mmm_coDh3l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}